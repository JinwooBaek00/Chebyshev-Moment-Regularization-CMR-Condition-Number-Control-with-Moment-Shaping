{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from __future__ import annotations\nimport os, math, random, pickle\nfrom dataclasses import dataclass\nfrom typing import Tuple, List, Dict, Optional\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Subset\nfrom torchvision import datasets, transforms\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nTensor = torch.Tensor\n\nclass CMR:\n    def __init__(self,\n                 K: int = 5,\n                 beta: float = 0.15,\n                 lam: float = 0.02,\n                 alpha1: float = 1.0,\n                 alpha2: float = 0.1,\n                 rho_spec: float = 0.5,\n                 warmup_steps: int = 1000,\n                 eps: float = 1e-6,\n                 delta: float = 1e-12,\n                 hutch_samples: int = 8,\n                 eig_method: str = 'gram_eigh',\n                 max_eigh_dim: int = 2048,\n                 mode: str = \"cmr\",\n                 device: Optional[torch.device] = None,\n                 verbose: bool = False):\n        assert K >= 3\n        assert 0 < rho_spec <= 1.0\n        assert mode == \"cmr\", \"This version only supports 'cmr' mode.\"\n        self.K = K\n        self.beta = beta\n        self.lam = lam\n        self.alpha1 = alpha1\n        self.alpha2 = alpha2\n        self.rho_spec = rho_spec\n        self.warmup_steps = warmup_steps\n        self.eps = eps\n        self.delta = delta\n        self.hutch_samples = hutch_samples\n        self.eig_method = eig_method\n        self.max_eigh_dim = max_eigh_dim\n        self.mode = mode\n        self.device = device\n        self.verbose = verbose\n        self._register_moment_weights()\n\n    def set_mode(self, mode: str):\n        assert mode == \"cmr\", \"This version only supports 'cmr' mode.\"\n        self.mode = mode\n\n    @torch.no_grad()\n    def lambda_t(self, t: int) -> float:\n        if self.warmup_steps <= 0:\n            return float(self.lam)\n        return float(self.lam) * min(1.0, float(t) / float(self.warmup_steps))\n\n    def step(self,\n             model: nn.Module,\n             task_loss: Tensor,\n             optimizer: torch.optim.Optimizer,\n             t: int,\n             retain_graph: bool = False) -> Dict[str, float]:\n\n        params = [p for p in model.parameters() if p.requires_grad]\n        if len(params) == 0:\n            return {}\n\n        dev = self._infer_device(params)\n\n        # 1) Task gradient\n        g_task = torch.autograd.grad(task_loss, params,\n                                      retain_graph=True,\n                                      create_graph=False,\n                                      allow_unused=True)\n\n        # 2) Spectral penalty\n        lam_t = self.lambda_t(t)\n        spec_loss, spec_stats = self._spectral_penalty(model, device=dev)\n        spec_loss = lam_t * spec_loss\n\n        g_spec = torch.autograd.grad(spec_loss, params,\n                                      retain_graph=retain_graph,\n                                      create_graph=False,\n                                      allow_unused=True)\n\n        # 3) Cap and mix\n        g_task_norm = self._global_norm(g_task)\n        g_spec_norm = self._global_norm(g_spec)\n        gamma = 0.0\n        if g_spec_norm > 0.0:\n            gamma = min(1.0, float(self.rho_spec * (g_task_norm / (g_spec_norm + self.delta))))\n\n        with torch.no_grad():\n            for p, gt, gs in zip(params, g_task, g_spec):\n                if p is None:\n                    continue\n                g = torch.zeros_like(p) if gt is None else gt.clone()\n                if gs is not None:\n                    g.add_(gs, alpha=gamma)\n                p.grad = g\n\n        optimizer.step()\n        optimizer.zero_grad(set_to_none=True)\n\n        return {\n            \"lambda_t\": lam_t,\n            \"gamma\": float(gamma),\n            \"g_task_norm\": float(g_task_norm),\n            \"g_spec_norm\": float(g_spec_norm),\n            \"spec_loss\": float(spec_loss.detach().cpu()),\n            \"cond_mean\": float(spec_stats[\"cond_mean\"]),\n            \"mom_mean\": float(spec_stats[\"mom_mean\"]),\n            \"mode\": self.mode,\n        }\n\n    def spectral_penalty(self, model: nn.Module) -> Tensor:\n        penalty, _ = self._spectral_penalty(model, device=self._infer_device([p for p in model.parameters() if p.requires_grad]))\n        return penalty\n\n    def _spectral_penalty(self, model: nn.Module, device: torch.device):\n        \n        if self.mode == \"cmr\":\n            cond_sum, mom_sum = [], []\n            for name, W in self._iter_weight_matrices(model):\n                W = W.to(device)\n                lam_max, lam_min = self._gram_eig_extrema(W, method=self._choose_eig_method(W.shape[1]))\n                cond = 0.5 * (torch.log(lam_max + 1e-30) - torch.log(lam_min + self.eps))\n                mom  = self._moment_proxy(W, lam_max=lam_max, lam_min=lam_min)\n                cond_sum.append(cond)\n                mom_sum.append(mom)\n\n            if not cond_sum:\n                zero = torch.zeros([], device=device)\n                return zero, dict(cond_mean=0.0, mom_mean=0.0)\n\n            cond_total = torch.stack(cond_sum).sum()\n            mom_total  = torch.stack(mom_sum).sum()\n            penalty = self.alpha1 * cond_total + self.alpha2 * mom_total\n            stats = dict(\n                cond_mean=float(torch.stack(cond_sum).mean().detach().cpu()),\n                mom_mean=float(torch.stack(mom_sum).mean().detach().cpu())\n            )\n            return penalty, stats\n        \n        else:\n            zero = torch.zeros([], device=device, requires_grad=True)\n            return zero, dict(cond_mean=0.0, mom_mean=0.0)\n\n    def _moment_proxy(self, W: Tensor, lam_max: Tensor, lam_min: Tensor) -> Tensor:\n        m, n = W.shape\n        device = W.device\n        c = 0.5 * (lam_max + lam_min)\n        d = torch.maximum(0.5 * (lam_max - lam_min), torch.tensor(self.eps, device=device))\n\n        def Ghat_mv(x: Tensor) -> Tensor:\n            y = W @ x\n            y = W.t() @ y\n            y = (y - c * x) / d\n            return y\n\n        S = self.hutch_samples\n        if S <= 0:\n            return torch.zeros([], device=device)\n\n        Z = torch.empty((n, S), device=device).bernoulli_().mul_(2.0).add_(-1.0)\n        V_prev = Z\n        V_cur  = Ghat_mv(Z)\n        s_vals: List[Tensor] = []\n\n        V_next = 2.0 * Ghat_mv(V_cur) - V_prev  # T2\n        k = 2\n        while k < self.K:\n            V_prev, V_cur = V_cur, V_next\n            k += 1\n            z_dot = (Z * V_cur).sum(dim=0)\n            s_k_est = z_dot.mean() / float(n)\n            if k >= 3:\n                s_vals.append(s_k_est)\n            if k < self.K:\n                V_next = 2.0 * Ghat_mv(V_cur) - V_prev\n\n        if not s_vals:\n            return torch.zeros([], device=device)\n        s_vec = torch.stack(s_vals)\n        w_vec = self._w_vec.to(device=device, dtype=s_vec.dtype)\n        return torch.sum(w_vec * (s_vec ** 2))\n\n    \n    # ---------------- eig helpers (for \"cmr\") ----------------\n    def _choose_eig_method(self, n: int) -> str:\n        if self.eig_method in (\"gram_eigh\", \"power\"):\n            return self.eig_method\n        return \"gram_eigh\" if n <= self.max_eigh_dim else \"power\"\n\n    def _gram_eig_extrema(self, W: Tensor, method: str = \"gram_eigh\"):\n        if method == \"gram_eigh\":\n            G = W.t() @ W\n            G = 0.5 * (G + G.t())\n            evals = torch.linalg.eigvalsh(G)\n            lam_min = torch.clamp(evals[0], min=0.0)\n            lam_max = torch.clamp(evals[-1], min=0.0)\n            return lam_max, lam_min\n\n        def G_mv(x: Tensor) -> Tensor:\n            return W.t() @ (W @ x)\n\n        lam_max = self._power_iteration(G_mv, W.shape[1], W.device, iters=30)\n        lam_min = self._inv_power_iteration_spd(G_mv, W.shape[1], W.device,\n                                                shift=self.eps, iters=12,\n                                                cg_iters=128, cg_tol=1e-5)\n        \n        lam_min = torch.clamp(lam_min, min=0.0)\n        lam_max = torch.clamp(lam_max, min=0.0)\n        return lam_max, lam_min\n\n    @torch.no_grad()\n    def _power_iteration(self, A_mv, n: int, device, iters: int = 30) -> Tensor:\n        v = torch.randn(n, device=device)\n        v = v / (v.norm() + 1e-30)\n        lam = torch.tensor(0.0, device=device)\n        for _ in range(iters):\n            w = A_mv(v)\n            normw = w.norm() + 1e-30\n            v = w / normw\n            lam = torch.dot(v, A_mv(v))\n        return lam\n\n    @torch.no_grad()\n    def _inv_power_iteration_spd(self, A_mv, n: int, device, shift: float,\n                                 iters: int = 12, cg_iters: int = 128, cg_tol: float = 1e-5) -> Tensor:\n        def solve_shifted(b: Tensor) -> Tensor:\n            x = torch.zeros_like(b)\n            r = b.clone()\n            p = r.clone()\n            rs_old = torch.dot(r, r)\n            for _ in range(cg_iters):\n                Ap = A_mv(p) + shift * p\n                alpha = rs_old / (torch.dot(p, Ap) + 1e-30)\n                x = x + alpha * p\n                r = r - alpha * Ap\n                rs_new = torch.dot(r, r)\n                if torch.sqrt(rs_new) < cg_tol:\n                    break\n                p = r + (rs_new / (rs_old + 1e-30)) * p\n                rs_old = rs_new\n            return x\n\n        v = torch.randn(n, device=device)\n        v = v / (v.norm() + 1e-30)\n        lam_inv = torch.tensor(0.0, device=device)\n        for _ in range(iters):\n            y = solve_shifted(v)\n            v = y / (y.norm() + 1e-30)\n            num = torch.dot(v, solve_shifted(v))\n            den = torch.dot(v, v) + 1e-30\n            lam_inv = num / den\n        return torch.relu(1.0 / (lam_inv + 1e-30) - shift)\n\n    def _iter_weight_matrices(self, model: nn.Module):\n        for name, module in model.named_modules():\n            if isinstance(module, nn.Linear) and module.weight is not None:\n                yield name, module.weight\n            elif isinstance(module, (nn.Conv1d, nn.Conv2d, nn.Conv3d)) and module.weight is not None:\n                W = module.weight\n                out_c = W.shape[0]\n                yield name, W.view(out_c, -1)\n\n    def _global_norm(self, grads: List[Optional[Tensor]]) -> float:\n        sq = 0.0\n        for g in grads:\n            if g is not None:\n                sq += float(torch.sum(g.detach()**2).cpu())\n        return math.sqrt(sq)\n\n    def _infer_device(self, params: List[Tensor]) -> torch.device:\n        if self.device is not None:\n            return torch.device(self.device)\n        for p in params:\n            if p is not None:\n                return p.device\n        return torch.device(\"cpu\")\n\n    def _register_moment_weights(self):\n        ks = torch.arange(3, self.K + 1, dtype=torch.float32)\n        self._w_vec = torch.exp(self.beta * (ks - 3.0))\n\ndef set_seed(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\ndef seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n\nclass MNISTDataModule:\n    def __init__(self, root=\"./data\", batch_size=128, ntrain=None, ntest=None, seed=42, num_workers=0):\n        self.root = root; self.batch_size = batch_size\n        self.ntrain = ntrain; self.ntest = ntest\n        self.seed = seed; self.num_workers = num_workers\n\n    def make_loaders(self) -> Tuple[DataLoader, DataLoader]:\n        tfm = transforms.Compose([transforms.ToTensor()])\n        train = datasets.MNIST(root=self.root, train=True, download=True, transform=tfm)\n        test  = datasets.MNIST(root=self.root, train=False, download=True, transform=tfm)\n\n        if self.ntrain is not None and self.ntrain < len(train):\n            rng = np.random.default_rng(self.seed)\n            idx = rng.choice(len(train), size=self.ntrain, replace=False)\n            train = Subset(train, idx.tolist())\n        if self.ntest is not None and self.ntest < len(test):\n            rng = np.random.default_rng(self.seed + 1)\n            idx = rng.choice(len(test), size=self.ntest, replace=False)\n            test = Subset(test, idx.tolist())\n\n        g = torch.Generator().manual_seed(self.seed)\n        train_loader = DataLoader(train, batch_size=self.batch_size, shuffle=True,\n                                  num_workers=self.num_workers, pin_memory=True,\n                                  worker_init_fn=seed_worker, generator=g)\n        test_loader  = DataLoader(test, batch_size=512, shuffle=False,\n                                  num_workers=self.num_workers, pin_memory=True)\n        return train_loader, test_loader\n\n# ---------------------- Model ----------------------\nclass DeepMLP(nn.Module):\n    def __init__(self, depth=15, width=256, num_classes=10,\n                 activation='tanh', input_scale=1.0,\n                 layer_gain=1.0, bias_shift=0.0):\n        super().__init__()\n        self.activation = activation\n        self.input_scale = float(input_scale)\n        self.layer_gain = float(layer_gain)\n        self.bias_shift = float(bias_shift)\n\n        self.collect_preacts = False\n        self.last_preacts = []\n\n        layers = []\n        in_dim = 28*28\n        for _ in range(depth - 1):\n            layers.append(nn.Linear(in_dim, width))\n            in_dim = width\n        layers.append(nn.Linear(in_dim, num_classes))\n        self.layers = nn.ModuleList(layers)\n\n    def forward(self, x):\n        x = x.view(x.size(0), -1) * self.input_scale\n        for i, lin in enumerate(self.layers):\n            x = lin(x)\n            if i < len(self.layers) - 1:\n                if self.layer_gain != 1.0: x = x * self.layer_gain\n                if self.bias_shift != 0.0: x = x + self.bias_shift\n                \n                if self.activation == 'tanh':\n                    x = torch.tanh(x)\n                elif self.activation == 'relu':\n                    x = F.relu(x, inplace=True)\n                else:\n                    x = F.gelu(x)\n        return x\n\n# ---------------------- Initializers ----------------------\nclass AdversarialInit:\n    @staticmethod\n    def _orthogonal_scaled_(lin: nn.Linear, scale: float):\n        nn.init.orthogonal_(lin.weight, gain=1.0)\n        with torch.no_grad():\n            lin.weight.mul_(float(scale))\n            if lin.bias is not None:\n                lin.bias.zero_()\n\n    @classmethod\n    def baseline(cls, model: nn.Module, vanish_scale: float = 0.06):\n        \"\"\"Applies orthogonal initialization scaled for vanishing.\"\"\"\n        scale = float(vanish_scale)\n        for m in model.modules():\n            if isinstance(m, nn.Linear):\n                cls._orthogonal_scaled_(m, scale)\n\n    @classmethod\n    def spectral_decay(cls, model: nn.Module, base=0.8, decay=0.95):\n        l = 0\n        for m in model.modules():\n            if isinstance(m, nn.Linear):\n                sc = float(base * (decay ** l))\n                cls._orthogonal_scaled_(m, sc)\n                l += 1\n\n# ---------------------- Metrics ----------------------\nclass Metrics:\n    @staticmethod\n    @torch.no_grad()\n    def evaluate(model: nn.Module, loader, device: torch.device) -> float:\n        model.eval()\n        correct, total = 0, 0\n        for x, y in loader:\n            x, y = x.to(device), y.to(device)\n            logits = model(x)\n            pred = logits.argmax(dim=1)\n            correct += (pred == y).sum().item()\n            total += y.numel()\n        return correct / max(1, total)\n\n    @staticmethod\n    @torch.no_grad()\n    def kappa_stats(model: nn.Module, eps: float = 1e-8) -> float:\n        vals = []\n        for m in model.modules():\n            if isinstance(m, nn.Linear):\n                s = torch.linalg.svdvals(m.weight.detach())\n                if s.numel() >= 1:\n                    vals.append(float(s[0].item() / (s[-1].item() + eps)))\n        return float(np.mean(vals)) if vals else float('nan')\n\n    @staticmethod\n    @torch.no_grad()\n    def kappa_stats_layerwise(model: nn.Module, eps: float = 1e-8):\n        out = []\n        for name, m in model.named_modules():\n            if isinstance(m, nn.Linear):\n                s = torch.linalg.svdvals(m.weight.detach())\n                kappa = float(s[0].item() / (s[-1].item() + eps))\n                out.append((name, kappa))\n        return out\n\n    @staticmethod\n    def grad_per_layer(model: nn.Module):\n        out = []\n        for name, m in model.named_modules():\n            if isinstance(m, nn.Linear):\n                g = 0.0\n                if m.weight.grad is not None:\n                    g = float(m.weight.grad.detach().norm().item())\n                out.append((name, g))\n        return out\n\n    @staticmethod\n    def summarize_layer_grads(layer_grads, tiny=1e-7):\n        vals = [g for _, g in layer_grads]\n        if not vals:\n            return dict(min=float('nan'), median=float('nan'), max=float('nan'), tiny_pct=float('nan'))\n        vals_sorted = sorted(vals)\n        n = len(vals)\n        tiny_pct = 100.0 * sum(v < tiny for v in vals) / n\n        return dict(min=min(vals), median=vals_sorted[n // 2], max=vals_sorted[-1], tiny_pct=tiny_pct)\n\n    @staticmethod\n    def grad_norm_params(params: List[torch.nn.Parameter]) -> float:\n        s = 0.0\n        for p in params:\n            if p.grad is not None:\n                s += p.grad.detach().pow(2).sum().item()\n        return float(s ** 0.5)\n\n    @staticmethod\n    @torch.no_grad()\n    def max_abs_moments(model: nn.Module, K: int = 5, eps: float = 1e-6) -> float:\n        M = 0.0\n        for m in model.modules():\n            if isinstance(m, nn.Linear):\n                G = m.weight.detach().T @ m.weight.detach()\n                n = G.shape[0]\n                evals = torch.linalg.eigvalsh(G)\n                lam_min, lam_max = float(evals[0].item()), float(evals[-1].item())\n                c = 0.5 * (lam_max + lam_min); d = max(0.5 * (lam_max - lam_min), eps)\n                I = torch.eye(n, device=G.device, dtype=G.dtype)\n                G_hat = (G - c*I) / d\n                Tkm1, Tk = I, G_hat\n                for k in range(2, K+1):\n                    Tkp1 = 2*G_hat @ Tk - Tkm1\n                    if k >= 3:\n                        s_k = torch.trace(Tkp1).item() / n\n                        M = max(M, abs(s_k))\n                    Tkm1, Tk = Tk, Tkp1\n        return float(M)\n\n# ---------------------- Trainer ----------------------\n@dataclass\nclass TrainConfig:\n    epochs: int = 5\n    lr: float = 1e-3\n    global_clip: float = 5.0\n    stable_lo: float = 1e-3\n    stable_hi: float = 3.0\n    # CMR\n    use_cmr: bool = False\n    lam: float = 0.03\n    lam_warmup: int = 3\n    rho_spec: float = 0.5\n    alpha1: float = 1.0\n    alpha2: float = 0.1\n    K: int = 5\n    beta: float = 0.15\n    eps: float = 1e-6\n    hutch_samples: int = 8\n    # CMR mode controls\n    cmr_mode: str = \"cmr\"\n    eig_method: str = \"gram_eigh\"\n    # logging\n    track_layerwise: bool = True\n    log_prefix: str = \"\"\n\nclass Trainer:\n    def __init__(self, model: nn.Module, device: torch.device, config: TrainConfig):\n        self.model = model.to(device)\n        self.device = device\n        self.cfg = config\n        self.opt = torch.optim.Adam(self.model.parameters(), lr=self.cfg.lr)\n        self.ce = nn.CrossEntropyLoss()\n        self.params = [p for p in self.model.parameters() if p.requires_grad]\n        self.cmr: Optional[CMR] = None\n        if self.cfg.use_cmr:\n            self.cmr = CMR(\n                K=self.cfg.K, beta=self.cfg.beta, lam=self.cfg.lam,\n                alpha1=self.cfg.alpha1, alpha2=self.cfg.alpha2,\n                rho_spec=self.cfg.rho_spec, warmup_steps=self.cfg.lam_warmup,\n                eps=self.cfg.eps, hutch_samples=self.cfg.hutch_samples,\n                eig_method=self.cfg.eig_method,\n                mode=self.cfg.cmr_mode,\n            )\n\n    def _warmup_lambda(self, epoch: int) -> float:\n        if not self.cfg.use_cmr:\n            return 0.0\n        t = max(0, min(epoch, self.cfg.lam_warmup))\n        return self.cfg.lam * (t / max(1, self.cfg.lam_warmup))\n\n    def fit(self, train_loader, test_loader) -> List[Dict]:\n        hist: List[Dict] = []\n        layerwise_hist: List[Dict] = [] if self.cfg.track_layerwise else None\n        global_step = 0\n\n        for ep in range(1, self.cfg.epochs + 1):\n            self.model.train()\n            batch_gn = []\n            stable_count, total_batches = 0, 0\n\n            front_grads_epoch, back_grads_epoch = [], []\n            tiny_hits_epoch, total_lin_epoch = 0, 0\n\n            lam_eff = self._warmup_lambda(ep - 1)\n            \n            self.model.collect_preacts = False\n\n            for x, y in train_loader:\n                x, y = x.to(self.device), y.to(self.device)\n                \n                logits = self.model(x)\n                loss_ce = self.ce(logits, y)\n\n                self.opt.zero_grad(set_to_none=True)\n                loss_ce.backward(retain_graph=True)\n                layer_grads_ce = Metrics.grad_per_layer(self.model)\n                stats_ce = Metrics.summarize_layer_grads(layer_grads_ce, tiny=1e-7)\n                vals = [g for _, g in layer_grads_ce]\n                if vals:\n                    mid = len(vals) // 2\n                    front_grads_epoch.append(float(np.mean(vals[:mid])))\n                    back_grads_epoch.append(float(np.mean(vals[mid:])))\n                    tiny_hits_epoch += sum(v < 1e-7 for v in vals)\n                    total_lin_epoch += len(vals)\n\n                self.opt.zero_grad(set_to_none=True)\n\n                if self.cmr is None or lam_eff <= 0.0:\n                    # Vanilla\n                    self.opt.zero_grad(set_to_none=True)\n                    loss_ce.backward() \n                    if self.cfg.global_clip and self.cfg.global_clip > 0:\n                        torch.nn.utils.clip_grad_norm_(self.params, max_norm=self.cfg.global_clip)\n                    gn = Metrics.grad_norm_params(self.params)\n                    batch_gn.append(gn)\n                    if self.cfg.stable_lo <= gn <= self.cfg.stable_hi:\n                        stable_count += 1\n                    total_batches += 1\n                    self.opt.step()\n                    self.opt.zero_grad(set_to_none=True)\n                else:\n                    # CMR\n                    stats = self.cmr.step(self.model, loss_ce, self.opt, global_step, retain_graph=False)\n                    \n                    gn_est = (stats[\"g_task_norm\"] ** 2 + (stats[\"gamma\"] * stats[\"g_spec_norm\"]) ** 2) ** 0.5\n                    batch_gn.append(gn_est)\n                    if self.cfg.stable_lo <= gn_est <= self.cfg.stable_hi:\n                        stable_count += 1\n                    total_batches += 1\n\n                global_step += 1\n\n            # epoch-end diagnostics\n            avg_grad = float(np.mean(batch_gn)) if batch_gn else float('nan')\n            stable_pct = 100.0 * stable_count / max(1, total_batches)\n            acc = Metrics.evaluate(self.model, test_loader, self.device)\n            kappa = Metrics.kappa_stats(self.model, eps=1e-8)\n            max_s = Metrics.max_abs_moments(self.model, K=self.cfg.K, eps=self.cfg.eps)\n            front_avg = float(np.mean(front_grads_epoch)) if front_grads_epoch else float('nan')\n            back_avg  = float(np.mean(back_grads_epoch))  if back_grads_epoch  else float('nan')\n            tiny_pct_epoch = 100.0 * tiny_hits_epoch / max(1, total_lin_epoch)\n\n            if self.cfg.track_layerwise:\n                kappa_layers = Metrics.kappa_stats_layerwise(self.model, eps=1e-8)\n                layerwise_hist.append({'epoch': ep, 'kappa_per_layer': kappa_layers})\n                worst = sorted(kappa_layers, key=lambda x: x[1], reverse=True)[:3]\n                worst_str = \", \".join([f\"{n.split('.')[-1]}:{k:.1f}\" for n, k in worst])\n                print(f\"{self.cfg.log_prefix} worst κ layers: {worst_str}\")\n\n            print(f\"{self.cfg.log_prefix} Epoch {ep:2d} | Test Acc: {acc:.4f} | Avg Grad: {avg_grad:.3e} \"\n                  f\"| FrontGrad: {front_avg:.1e} | BackGrad: {back_avg:.1e} | tiny%: {tiny_pct_epoch:4.1f} \"\n                  f\"| Stable %: {stable_pct:5.1f} | Avg κ: {kappa:.1f} | Max|s_k|: {max_s:.3f} | lam={lam_eff:.4f}\")\n\n            hist.append(dict(epoch=ep, acc=acc, avg_grad=avg_grad, stable_pct=stable_pct,\n                             kappa=kappa, max_abs_s=max_s, lam=lam_eff,\n                             layer_min_grad=stats_ce['min'], layer_med_grad=stats_ce['median'],\n                             layer_max_grad=stats_ce['max'], layer_tiny_pct=tiny_pct_epoch,\n                             front_grad_avg=front_avg, back_grad_avg=back_avg))\n\n        if self.cfg.track_layerwise and layerwise_hist:\n            safe_prefix = self.cfg.log_prefix.replace(\" \", \"_\")\n            filename = f\"{safe_prefix}_layerwise.pkl\"\n            with open(filename, 'wb') as f:\n                pickle.dump(layerwise_hist, f)\n            print(f\"Layer-wise data saved to {filename}\\n\")\n\n        return hist\n\n# ---------------------- Experiment Runner ----------------------\n@dataclass\nclass ExperimentConfig:\n    # runtime\n    seed: int = 42\n    epochs: int = 5\n    batch_size: int = 128\n    ntrain: Optional[int] = None\n    ntest: Optional[int] = None\n    num_workers: int = 0\n    # model\n    lr: float = 1e-3\n    depth: int = 15\n    width: int = 256\n    activation: str = \"tanh\"\n    track_layerwise: bool = True\n    # vanish controls\n    vanish_scale: float = 0.06\n    # CMR hyperparams\n    lam: float = 0.03\n    lam_warmup: int = 3\n    rho_spec: float = 0.5\n    alpha1: float = 1.0\n    alpha2: float = 0.1\n    K: int = 5\n    beta: float = 0.15\n    eps: float = 1e-6\n    hutch_samples: int = 8\n    # safety & logging\n    global_clip: float = 5.0\n    stable_lo: float = 1e-3\n    stable_hi: float = 3.0\n    # CMR mode\n    cmr_mode: str = \"cmr\"\n    eig_method: str = \"gram_eigh\"\n\nclass ExperimentRunner:\n    def __init__(self, cfg: ExperimentConfig):\n        self.cfg = cfg\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    def _build_model(self) -> DeepMLP:\n        return DeepMLP(depth=self.cfg.depth, width=self.cfg.width,\n                       activation=self.cfg.activation, input_scale=1.0)\n\n    def _apply_init(self, model: nn.Module):\n        AdversarialInit.baseline(model, vanish_scale=self.cfg.vanish_scale)\n\n    def run(self):\n        set_seed(self.cfg.seed)\n        dm = MNISTDataModule(batch_size=self.cfg.batch_size, ntrain=self.cfg.ntrain, ntest=self.cfg.ntest,\n                             seed=self.cfg.seed, num_workers=self.cfg.num_workers)\n        train_loader, test_loader = dm.make_loaders()\n\n        print(\"DEVICE:\", self.device)\n        print(\"CFG summary:\", {'epochs': self.cfg.epochs, 'lr': self.cfg.lr,\n                              'depth': self.cfg.depth, 'width': self.cfg.width,\n                              'CMR_MODE': self.cfg.cmr_mode})\n\n        print(\"\\nRunning VANISH scenario\")\n\n        # VANILLA\n        set_seed(self.cfg.seed)\n        vanilla = self._build_model()\n        self._apply_init(vanilla)\n        trainer_van = Trainer(\n            vanilla, self.device,\n            TrainConfig(\n                epochs=self.cfg.epochs, lr=self.cfg.lr,\n                global_clip=self.cfg.global_clip, stable_lo=self.cfg.stable_lo, stable_hi=self.cfg.stable_hi,\n                use_cmr=False, track_layerwise=self.cfg.track_layerwise, log_prefix=\"--Vanilla--\",\n                # Pass defaults\n                lam_warmup=self.cfg.lam_warmup, \n                hutch_samples=self.cfg.hutch_samples,\n                eig_method=self.cfg.eig_method\n            )\n        )\n        hist_v = trainer_van.fit(train_loader, test_loader)\n\n        # CMR\n        set_seed(self.cfg.seed)\n        train_loader, test_loader = dm.make_loaders()\n        cmr_model = self._build_model()\n        self._apply_init(cmr_model)\n        trainer_cmr = Trainer(\n            cmr_model, self.device,\n            TrainConfig(\n                epochs=self.cfg.epochs, lr=self.cfg.lr,\n                global_clip=self.cfg.global_clip, stable_lo=self.cfg.stable_lo, stable_hi=self.cfg.stable_hi,\n                use_cmr=True, lam=self.cfg.lam, lam_warmup=self.cfg.lam_warmup, rho_spec=self.cfg.rho_spec,\n                alpha1=self.cfg.alpha1, alpha2=self.cfg.alpha2, K=self.cfg.K, beta=self.cfg.beta, eps=self.cfg.eps,\n                hutch_samples=self.cfg.hutch_samples,\n                cmr_mode=self.cfg.cmr_mode, eig_method=self.cfg.eig_method,\n                track_layerwise=self.cfg.track_layerwise, log_prefix=\"--CMR--\"\n            )\n        )\n        hist_c = trainer_cmr.fit(train_loader, test_loader)\n\n        # Plot\n        try:\n            df_v = pd.DataFrame(hist_v); df_c = pd.DataFrame(hist_c)\n            plt.figure(figsize=(8,4))\n            plt.plot(df_v['epoch'], df_v['acc'], 'o--', label='Vanilla')\n            plt.plot(df_c['epoch'], df_c['acc'], 's-',  label=f'CMR (mode={self.cfg.cmr_mode})')\n            plt.xlabel('Epoch'); plt.ylabel('Test Acc'); plt.title(f'VANISH: Test Acc (CMR Mode: {self.cfg.cmr_mode})')\n            plt.grid(True, ls='--', lw=0.5); plt.legend()\n            plt.tight_layout(); \n            plot_filename = f'vanish_acc_{self.cfg.cmr_mode}.png'\n            plt.savefig(plot_filename, dpi=150)\n            print(f\"Saved plot: {plot_filename}\")\n        except Exception as e:\n            print(\"Plotting skipped:\", e)\n\n# ---------------------- Main ----------------------\nif __name__ == \"__main__\":\n    \n    print(\"=\"*60)\n    print(\" CMR Vanish Experiment\")\n    print(\"=\"*60)\n    print(\"1. Run VANISH with CMR\")\n    print(\"2. Exit\")\n    choice = input(\"Select an option (1 or 2): \").strip()\n\n    if choice == '1':\n        print(\"\\nRunning 'cmr mode...\")\n        cmr_mode_to_run = \"cmr\"\n        eig_method_to_run = \"gram_eigh\"\n    \n    elif choice == '2':\n        print(\"Exiting.\")\n        exit()\n    \n    else:\n        print(\"Invalid choice, exiting.\")\n        exit()\n        \n    CFG = ExperimentConfig(\n        seed=42, epochs=10, batch_size=128, ntrain=None, ntest=None, num_workers=0,\n        lr=1e-3, depth=15, width=256, activation=\"tanh\",\n        track_layerwise=True,\n        vanish_scale=0.06,\n        \n        # CMR Base\n        lam=0.03, \n        lam_warmup=3,\n        rho_spec=0.5, \n        alpha1=1.0,\n        alpha2=0.1,\n        K=5, beta=0.15, eps=1e-6,\n        hutch_samples=8,\n        \n        # Safety\n        global_clip=5.0, stable_lo=1e-3, stable_hi=3.0,\n        \n        cmr_mode=cmr_mode_to_run,\n        eig_method=eig_method_to_run,\n    )\n    \n    ExperimentRunner(CFG).run()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-04T22:48:12.956365Z","iopub.execute_input":"2025-11-04T22:48:12.957130Z","iopub.status.idle":"2025-11-04T22:49:37.842487Z","shell.execute_reply.started":"2025-11-04T22:48:12.957057Z","shell.execute_reply":"2025-11-04T22:49:37.840986Z"}},"outputs":[{"name":"stdout","text":"============================================================\n CMR Vanish Experiment\n============================================================\n1. Run VANISH with CMR\n2. Exit\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Select an option (1 or 2):  1\n"},{"name":"stdout","text":"\nRunning 'cmr mode...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 9.91M/9.91M [00:00<00:00, 58.3MB/s]\n100%|██████████| 28.9k/28.9k [00:00<00:00, 1.70MB/s]\n100%|██████████| 1.65M/1.65M [00:00<00:00, 14.1MB/s]\n100%|██████████| 4.54k/4.54k [00:00<00:00, 4.03MB/s]\n","output_type":"stream"},{"name":"stdout","text":"DEVICE: cpu\nCFG summary: {'epochs': 10, 'lr': 0.001, 'depth': 15, 'width': 256, 'CMR_MODE': 'cmr'}\n\n*** Running VANISH scenario ***\n--Vanilla-- worst κ layers: 13:36913.8, 10:13083.8, 6:12989.6\n--Vanilla-- Epoch  1 | Test Acc: 0.2101 | Avg Grad: 7.451e-01 | FrontGrad: 1.0e-01 | BackGrad: 1.0e-01 | tiny%:  8.8 | Stable %:  95.9 | Avg κ: 6368.6 | Max|s_k|: 0.993 | lam=0.0000\n--Vanilla-- worst κ layers: 12:60544.8, 11:41767.2, 9:11154.4\n--Vanilla-- Epoch  2 | Test Acc: 0.2074 | Avg Grad: 4.777e-01 | FrontGrad: 2.8e-03 | BackGrad: 8.3e-02 | tiny%:  2.2 | Stable %:  99.8 | Avg κ: 9948.7 | Max|s_k|: 0.993 | lam=0.0000\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/1546331983.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    809\u001b[0m     )\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m     \u001b[0mExperimentRunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_37/1546331983.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    728\u001b[0m             )\n\u001b[1;32m    729\u001b[0m         )\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0mhist_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer_van\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;31m# CMR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_37/1546331983.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_loader, test_loader)\u001b[0m\n\u001b[1;32m    601\u001b[0m                         \u001b[0mstable_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m                     \u001b[0mtotal_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m                             )\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    242\u001b[0m             )\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    245\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    877\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as_real\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0mexp_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as_real\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":1}]}